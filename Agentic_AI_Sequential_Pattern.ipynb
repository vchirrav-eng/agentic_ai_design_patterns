{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvw35w2SDrIu"
      },
      "outputs": [],
      "source": [
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GEMINI_API_KEY'] = 'MY_API_KEY'"
      ],
      "metadata": {
        "id": "1dIa18RtEpda"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "\n",
        "# Initialize the Google GenAI client\n",
        "# It automatically picks up the GEMINI_API_KEY environment variable\n",
        "client = genai.Client()\n",
        "\n",
        "# Set the model we want our agents to use\n",
        "MODEL_ID = 'gemini-2.5-flash'\n",
        "\n",
        "def task_a_extractor_agent(raw_data: str) -> str:\n",
        "    \"\"\"Subagent A: Extracts key entities from raw data.\"\"\"\n",
        "    print(\"-> [Task A] Extracting data...\")\n",
        "    prompt = f\"Extract only the main subjects and verbs from this text:\\n\\n{raw_data}\"\n",
        "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
        "    return response.text\n",
        "\n",
        "def task_b_formatter_agent(extracted_data: str) -> str:\n",
        "    \"\"\"Subagent B: Formats the extracted data into a neat list.\"\"\"\n",
        "    print(\"-> [Task B] Formatting data...\")\n",
        "    prompt = f\"Format the following raw extracted data into a clean, bulleted Markdown list:\\n\\n{extracted_data}\"\n",
        "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
        "    return response.text\n",
        "\n",
        "def task_c_summarizer_agent(formatted_data: str) -> str:\n",
        "    \"\"\"Subagent C: Adds a final summary to the formatted list.\"\"\"\n",
        "    print(\"-> [Task C] Adding final summary...\")\n",
        "    prompt = f\"Add a brief, one-sentence summary to the top of this list:\\n\\n{formatted_data}\"\n",
        "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
        "    return response.text\n",
        "\n",
        "def sequential_workflow_agent(user_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    The orchestrator agent.\n",
        "    It follows predefined, linear logic without asking an LLM for routing.\n",
        "    \"\"\"\n",
        "    print(\"Starting Sequential Workflow...\")\n",
        "\n",
        "    # 1. Pass the user prompt to Task A\n",
        "    result_a = task_a_extractor_agent(user_prompt)\n",
        "\n",
        "    # 2. Pass Task A's output directly to Task B\n",
        "    result_b = task_b_formatter_agent(result_a)\n",
        "\n",
        "    # 3. Pass Task B's output directly to Task C\n",
        "    final_result = task_c_summarizer_agent(result_b)\n",
        "\n",
        "    # Return the final chain result to the user\n",
        "    return final_result\n",
        "\n",
        "# --- Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # The user's initial input\n",
        "    raw_user_input = (\n",
        "        \"In 2024, the company saw a massive surge in revenue up to $50 million. \"\n",
        "        \"However, operating costs also skyrocketed to $40 million due to supply chain issues. \"\n",
        "        \"The CEO, Jane Doe, announced a new restructuring plan for 2025 to optimize margins.\"\n",
        "    )\n",
        "\n",
        "    # Trigger the sequential agent\n",
        "    final_output = sequential_workflow_agent(raw_user_input)\n",
        "\n",
        "    print(\"\\n=== Final Response to User ===\")\n",
        "    print(final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHpuOrDHEOYZ",
        "outputId": "326d8e3b-9204-487e-f513-4c9bb659d326"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Sequential Workflow...\n",
            "-> [Task A] Extracting data...\n",
            "-> [Task B] Formatting data...\n",
            "-> [Task C] Adding final summary...\n",
            "\n",
            "=== Final Response to User ===\n",
            "The company's CEO made an announcement amidst skyrocketing operating costs.\n",
            "\n",
            "*   **company** saw\n",
            "*   **operating costs** skyrocketed\n",
            "*   **CEO** announced\n"
          ]
        }
      ]
    }
  ]
}